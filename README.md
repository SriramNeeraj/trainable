To sum-up: 'trainable parameters' are those which value is modified according to their gradient (the derivative of the error/loss/cost relative to the parameter), whereas 'non-trainable parameters' are those which value is not optimized according to their gradient.The only built-in layer that has non-trainable weights is the BatchNormalization layer. It uses non-trainable weights to keep track of the mean and variance of its inputs during training.Trainable parameters are those which value is adjusted/modified during training as per their gradient. In Batch Normalization layer we have below mentioned trainable params: gamma: It's a scaling factor. beta: a learned offset factor.Trainable parameters between input layer and first hidden layer: 5×8 + 8 = 48. Trainable parameters between first and second hidden layers: 8×4 + 4 = 36. Trainable parameters between second hidden layer and output layer: 4×3 + 3 = 15. Total number of trainable parameters of the neural net: 48 + 36 + 15 = 99.Thus number of parameters = 0. CONV layer: This is where CNN learns, so certainly we'll have weight matrices. To calculate the learnable parameters here, all we have to do is just multiply the by the shape of width m, height n, previous layer's filters d and account for all such filters k in the current layerReaching 6 parameters is below average, so the common sense that set the bar around 3 or 4, and “for sure, nothing beyond 6”, can be read on the actual coding. Methods with 10 arguments or more appear in less that 20% of projects. That's still quite a lot.Thus, this feed-forward neural network has 94 connections in all and thus 94 trainable parameters.Keras Model is trainable by default - you have two means of freezing all the weights: model. trainable = False before compiling the model. for layer in modelConvolutional_2 : As convolutional_1 already learned 32 filters. So the number of trainable parameters in this layer is 3 * 3 * 32 + 1 * 32 = 9248 and so on. Max_pooling_2d: This layer is used to reduce the input image size.You can pass a trainable argument (boolean) to a layer constructor to set a layer to be non-trainable. Additionally, you can set the trainable property of a layer to True or False after instantiation. For this to take effect, you will need to call compile() on your model after modifying the trainable propertybiases. So in total, the amount of parameters in this neural network is 13002.The “Param #” column shows you the number of parameters that are trained for each layer. The total number of parameters is shown at the end, which is equal to the number of trainable and non-trainable parameters. In this model, all the layers are trainableNumber of parameters in a MLP network

 So, 20 weights and 5 bias terms (20+5=25) are trainable parameters between this two layers. In the Similar way, we can compute the number of trainable parameters between hidden layer-1 and hidden layer-2 and also between hidden layer-2 and output layer
